{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Workspace\\Repos\\ParameterEstimator\\env\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">899</span> (3.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m899\u001b[0m (3.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">899</span> (3.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m899\u001b[0m (3.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the number of input features, hidden units, and output classes\n",
    "input_features = 10  # Example: 10 input features\n",
    "hidden_units = 64    # Example: 64 hidden units\n",
    "output_classes = 2   # Example: binary classification (2 classes)\n",
    "\n",
    "# Create the sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the input layer and the hidden layer\n",
    "model.add(Dense(hidden_units, input_shape=(input_features,), activation='relu'))\n",
    "\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(3, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "model.compile(optimizer='adam', loss=custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Workspace\\Repos\\ParameterEstimator\\env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:188: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ custom_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CustomLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ custom_layer (\u001b[38;5;33mCustomLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m)                 │             \u001b[38;5;34m3\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class CustomLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomLayer, self).__init__(**kwargs)\n",
    "        # Define 3 custom neurons (weights)\n",
    "        self.neuron1 = self.add_weight(name='neuron1', shape=(1,), initializer='uniform', trainable=True)\n",
    "        self.neuron2 = self.add_weight(name='neuron2', shape=(1,), initializer='uniform', trainable=True)\n",
    "        self.neuron3 = self.add_weight(name='neuron3', shape=(1,), initializer='uniform', trainable=True)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # You can perform additional setup here if needed, and create more weights if necessary\n",
    "        super(CustomLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define the computation of the layer using the custom neurons\n",
    "        # Assuming 'inputs' is a tensor with shape (batch_size, 3) where each column corresponds to the output of a neuron\n",
    "        output = self.neuron1 * inputs[:, 0] + self.neuron2 * inputs[:, 1] + self.neuron3 * inputs[:, 2]\n",
    "        return output\n",
    "\n",
    "# Example usage:\n",
    "# Let's assume the input tensor has shape (batch_size, 3)\n",
    "input_tensor = tf.keras.Input(shape=(3,))\n",
    "custom_layer = CustomLayer()\n",
    "output_tensor = custom_layer(input_tensor)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Summary of the model to check the custom layer\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.         0.         6.7167606]\n",
      " [ 0.         0.        17.390661 ]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Define the input shape (number of features)\n",
    "input_shape = (4,)  # For example, 4 features per sample\n",
    "\n",
    "# Create a Sequential model and add a Dense layer\n",
    "model = Sequential([\n",
    "    Dense(units=3, activation='relu', input_shape=input_shape)  # Dense layer with 3 neurons\n",
    "])\n",
    "\n",
    "# Optionally, compile the model if you plan to train it\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Create a sample input tensor (e.g., a batch of 2 samples with 4 features each)\n",
    "input_tensor = tf.constant([[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0]])\n",
    "\n",
    "# Get the forward pass output of the dense layer (also known as the layer's activation)\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "# Print the output tensor\n",
    "print(output_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"relu\",\n",
    "                                             kernel_initializer=\"he_normal\")\n",
    "                       for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualRegressor(tf.keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = tf.keras.layers.Dense(30, activation=\"relu\",\n",
    "                                             kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_binomial_layer(x):\n",
    "    \"\"\"\n",
    "    Lambda function for generating negative binomial parameters\n",
    "    n and p from a Dense(2) output.\n",
    "    Assumes tensorflow 2 backend.\n",
    "    \n",
    "    Usage\n",
    "    -----\n",
    "    outputs = Dense(2)(final_layer)\n",
    "    distribution_outputs = Lambda(negative_binomial_layer)(outputs)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tf.Tensor\n",
    "        output tensor of Dense layer\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    out_tensor : tf.Tensor\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the number of dimensions of the input\n",
    "    num_dims = len(x.get_shape())\n",
    "    \n",
    "    # Separate the parameters\n",
    "    mu, sd, alpha = tf.unstack(x, num=3, axis=-1)\n",
    "    \n",
    "    # Add one dimension to make the right shape\n",
    "    mu = tf.expand_dims(mu, -1)\n",
    "    sd = tf.expand_dims(sd, -1)\n",
    "    alpha = tf.expand_dims(alpha, -1)\n",
    "        \n",
    "    # # Apply a softplus to make positive\n",
    "    # n = tf.keras.activations.softplus(n)\n",
    "    \n",
    "    # # Apply a sigmoid activation to bound between 0 and 1\n",
    "    # p = tf.keras.activations.sigmoid(p)\n",
    "    \n",
    "    # sd = tf.keras.activations.relu(sd)\n",
    "    sd = tf.nn.relu(sd)\n",
    "\n",
    "    # Join back together again\n",
    "    out_tensor = tf.concat((mu, sd, alpha), axis=num_dims-1)\n",
    "\n",
    "    return out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skewnorm\n",
    "\n",
    "def negative_binomial_loss(y_true, y_pred, x):\n",
    "    \"\"\"\n",
    "    Negative binomial loss function.\n",
    "    Assumes tensorflow backend.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : tf.Tensor\n",
    "        Ground truth values of predicted variable.\n",
    "    y_pred : tf.Tensor\n",
    "        n and p values of predicted distribution.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    nll : tf.Tensor\n",
    "        Negative log likelihood.\n",
    "    \"\"\"\n",
    "\n",
    "    # Separate the parameters\n",
    "    mu, sd, alpha = tf.unstack(y_pred, num=3, axis=-1)\n",
    "    \n",
    "    # Add one dimension to make the right shape\n",
    "    mu = tf.expand_dims(mu, -1)\n",
    "    sd = tf.expand_dims(sd, -1)\n",
    "    alpha = tf.expend_dims(alpha, -1)\n",
    "    \n",
    "    # Calculate the negative log likelihood\n",
    "    L = (\n",
    "        y_true - skewnorm.pdf(x, alpha, mu, sd)\n",
    "    )           \n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "class ResidualRegressor(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = tf.keras.layers.Dense(64, activation=\"relu\",\n",
    "                                             kernel_initializer=\"he_normal\")\n",
    "        self.hidden2 = tf.keras.layers.Dense(3, activation=\"relu\",\n",
    "                                             kernel_initializer=\"he_normal\")\n",
    "        # self.paramLayer = negative_binomial_layer()\n",
    "        # self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        Z = self.hidden2(Z)\n",
    "        Z = Lamba(negative_binomial_layer)(Z)\n",
    "        # for _ in range(1 + 3):\n",
    "        #     Z = self.block1(Z)\n",
    "        # Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "# from tensorflow.keras.models import Model\n",
    "\n",
    "# Layer1 = Dense(1)\n",
    "# output1 = Layer1(inputs)\n",
    "# output2 = Layer2(output1)\n",
    "\n",
    "# # Predict the parameters of a negative binomial distribution\n",
    "# outputs = Dense(2)(output2)\n",
    "# distribution_outputs = Lambda(negative_binomial_layer)(outputs)\n",
    "\n",
    "# # Construct model\n",
    "# model = Model(inputs=inputs, outputs=distribution_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = ResidualRegressor()\n",
    "model.compile(loss=negative_binomial_loss, optimizer=\"adam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
